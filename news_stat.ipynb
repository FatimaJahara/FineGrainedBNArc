{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvi9NytP4/TDYzv8NibRBU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FatimaJahara/FineGrainedBNArc/blob/main/news_stat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXDUHa7QWmQP",
        "outputId": "31fb4095-5a32-4e79-8e39-52b76a335d1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ktrain\n",
            "  Downloading ktrain-0.36.0.tar.gz (25.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.3/25.3 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from ktrain) (1.2.2)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from ktrain) (3.7.1)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from ktrain) (1.5.3)\n",
            "Requirement already satisfied: fastprogress>=0.1.21 in /usr/local/lib/python3.10/dist-packages (from ktrain) (1.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ktrain) (2.27.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from ktrain) (1.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ktrain) (23.1)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from ktrain) (0.42.1)\n",
            "Collecting cchardet\n",
            "  Downloading cchardet-2.1.7.tar.gz (653 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m653.6/653.6 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from ktrain) (4.0.0)\n",
            "Collecting syntok>1.3.3\n",
            "  Downloading syntok-1.4.4-py3-none-any.whl (24 kB)\n",
            "Collecting tika\n",
            "  Downloading tika-2.6.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers>=4.17.0\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.98-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras_bert>=0.86.0\n",
            "  Downloading keras-bert-0.89.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting whoosh\n",
            "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.8/468.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras_bert>=0.86.0->ktrain) (1.22.4)\n",
            "Collecting keras-transformer==0.40.0\n",
            "  Downloading keras-transformer-0.40.0.tar.gz (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-pos-embd==0.13.0\n",
            "  Downloading keras-pos-embd-0.13.0.tar.gz (5.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-multi-head==0.29.0\n",
            "  Downloading keras-multi-head-0.29.0.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-layer-normalization==0.16.0\n",
            "  Downloading keras-layer-normalization-0.16.0.tar.gz (3.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-position-wise-feed-forward==0.8.0\n",
            "  Downloading keras-position-wise-feed-forward-0.8.0.tar.gz (4.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-embed-sim==0.10.0\n",
            "  Downloading keras-embed-sim-0.10.0.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-self-attention==0.51.0\n",
            "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->ktrain) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->ktrain) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->ktrain) (0.11.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->ktrain) (8.4.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->ktrain) (4.39.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->ktrain) (2.8.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->ktrain) (1.0.7)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->ktrain) (2022.7.1)\n",
            "Requirement already satisfied: regex>2016 in /usr/local/lib/python3.10/dist-packages (from syntok>1.3.3->ktrain) (2022.10.31)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.17.0->ktrain) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.17.0->ktrain) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.17.0->ktrain) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->ktrain) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->ktrain) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ktrain) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ktrain) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ktrain) (1.26.15)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->ktrain) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->ktrain) (1.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tika->ktrain) (67.7.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers>=4.17.0->ktrain) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers>=4.17.0->ktrain) (4.5.0)\n",
            "Building wheels for collected packages: ktrain, keras_bert, keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-pos-embd, keras-position-wise-feed-forward, keras-self-attention, cchardet, langdetect, tika\n",
            "  Building wheel for ktrain (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ktrain: filename=ktrain-0.36.0-py3-none-any.whl size=25317525 sha256=94b80ccc667737ad10cc2cefe501a26fae70b600064bd7d7a75befa4fd6f6b25\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/05/03/a539d528fa2c1c6722a241f30f2b649fa2590c4b0324a6b058\n",
            "  Building wheel for keras_bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras_bert: filename=keras_bert-0.89.0-py3-none-any.whl size=33517 sha256=4e2de9f0f342764c36fc23696fb01e6c7b0f5d0cda5a1606848c1d0c5e5865cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/0c/04/646b6fdf6375911b42c8d540a8a3fda8d5d77634e5dcbe7b26\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.40.0-py3-none-any.whl size=12303 sha256=38b1186e6abdc04800b8b975a80b6e8b6d2af78316022b148307d1cefced3c99\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/cb/22/75a0ad376129177f7c95c0d91331a18f5368fd657f4035ba7c\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.10.0-py3-none-any.whl size=3959 sha256=8cc59afb9b7cf15e71998f82fb9a7115d3f8bf2cb4a76c50e344cc9dcab19e6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/32/c7/fd35d0d1b840a6c7cbd4343f808d10d0f7b87d271a4dbe796f\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.16.0-py3-none-any.whl size=4666 sha256=16cb0886348615b96d7af6c18c9872076b3453041bcaf473eede42e062b08fa0\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/3a/4b/21db23c0cc56c4b219616e181f258eb7c57d36cc5d056fae9a\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.29.0-py3-none-any.whl size=14992 sha256=14c3cf6df44b270fa1f8b4b25ec75328cbd9438ad8437d04c6c1b91312cf1801\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/23/4b/06d7ae21714f70fcc25b48f972cc8e5e7f4b6b764a038b509d\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.13.0-py3-none-any.whl size=6958 sha256=e61d0384eb26c38467aac474db0018ec501f037af61c84264c07415cd0b7c527\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/07/1b/b1ca47b6ac338554b75c8f52c54e6a2bfbe1b07d79579979a4\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.8.0-py3-none-any.whl size=4983 sha256=d5783a616e69d6e3cb4496509511682b17150fd426d9109457b78ac7ca3a717f\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/6a/04/d1706a53b23b2cb5f9a0a76269bf87925daa1bca09eac01b21\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18910 sha256=011f3d2732337d08561241f593c9e4028d7ed29e3b4fade130f4699b3e42cacf\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/f7/24/607b483144fb9c47b4ba2c5fba6b68e54aeee2d5bf6c05302e\n",
            "  Building wheel for cchardet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cchardet: filename=cchardet-2.1.7-cp310-cp310-linux_x86_64.whl size=261602 sha256=91555aa15f2eef03e21ac2c6755a7a8f49d52a6ca03482cc46fac146e5cbfe67\n",
            "  Stored in directory: /root/.cache/pip/wheels/ee/e0/ab/e01326f15c59438d080b1496dbab8091e952ec72f35e3c437e\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993243 sha256=559057c73f1010a4f24b55881d240d188b48f69ea7cdb6c096a27c83719cc788\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tika: filename=tika-2.6.0-py3-none-any.whl size=32641 sha256=2b53a2d5311b7d6ac4173a09cd83ffb34e6fb45dbcaf3cd4ef0e8c4777a7200b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/71/c7/b757709531121b1700cffda5b6b0d4aad095fb507ec84316d0\n",
            "Successfully built ktrain keras_bert keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-pos-embd keras-position-wise-feed-forward keras-self-attention cchardet langdetect tika\n",
            "Installing collected packages: whoosh, tokenizers, sentencepiece, cchardet, syntok, langdetect, keras-self-attention, keras-position-wise-feed-forward, keras-pos-embd, keras-layer-normalization, keras-embed-sim, tika, keras-multi-head, huggingface-hub, transformers, keras-transformer, keras_bert, ktrain\n",
            "Successfully installed cchardet-2.1.7 huggingface-hub-0.14.1 keras-embed-sim-0.10.0 keras-layer-normalization-0.16.0 keras-multi-head-0.29.0 keras-pos-embd-0.13.0 keras-position-wise-feed-forward-0.8.0 keras-self-attention-0.51.0 keras-transformer-0.40.0 keras_bert-0.89.0 ktrain-0.36.0 langdetect-1.0.9 sentencepiece-0.1.98 syntok-1.4.4 tika-2.6.0 tokenizers-0.13.3 transformers-4.28.1 whoosh-2.7.4\n",
            "CPU times: user 4.89 s, sys: 682 ms, total: 5.58 s\n",
            "Wall time: 59.2 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "!pip install ktrain\n",
        "import ktrain\n",
        "from ktrain import text\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score, roc_curve, precision_recall_curve\n",
        "\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOhvfMQQWnCg",
        "outputId": "c2096edc-9f53-49df-8378-68bc0df86282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = pd.read_csv('/content/drive/My Drive/Data/Accident_journal.csv')\n",
        "crime = pd.read_csv('/content/drive/My Drive/Data/Crime_journal.csv')\n",
        "ent = pd.read_csv('/content/drive/My Drive/Data/Entertainment_journal.csv')\n",
        "sports = pd.read_csv('/content/drive/My Drive/Data/Sports_journal.csv')"
      ],
      "metadata": {
        "id": "lQ3k6F1vXKmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "PP5Ov_kpXRVY",
        "outputId": "3220da0e-8c46-4a4c-efdb-68b3571f0da0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          ID                                              Title  \\\n",
              "0          1          পাখির আঘাতে ড্যাশ উড়োজাহাজের জরুরি অবতরণ    \n",
              "1          2    চট্টগ্রামে বিমানে যান্ত্রিক ত্রুটি জরুরি অবতরণ    \n",
              "2          3                   বিমানের ড্যাশ এইটের জরুরি অবতরণ    \n",
              "3          4     উড়োজাহাজেই মৃত্যু ওমানপ্রবাসীর  ঘটেছি ফ্লাইটে    \n",
              "4          5                                        অভ্র করবেন    \n",
              "...      ...                                                ...   \n",
              "11842  11843                সাঙ্গু নদে ডুবে কলেজছাত্রের মৃত্যু    \n",
              "11843  11844                        পানিতে ডুবে গৃহবধূর মৃত্যু    \n",
              "11844  11845                 উদ্ধার হননি লঞ্চ থে‌কে প‌ড়ে নারী    \n",
              "11845  11846                    উদ্ধার  জেলে খোঁজ মেলেনি নৌকার    \n",
              "11846  11847   বঙ্গোপসাগরে জাহাজের ধাক্কায় ট্রলারডুবি পাঁচ ...   \n",
              "\n",
              "                                             Description  Label  \n",
              "0      বিমান বাংলাদেশ এয়ারলাইনসের ড্যাশ মডেলের উড়োজাহ...    Air  \n",
              "1      আকাশে ওড়ার  মিনিট চট্টগ্রাম শাহ আমানত আন্তর্জ...    Air  \n",
              "2      হজরত শাহজালাল আন্তর্জাতিক বিমানবন্দরে বিমান বা...    Air  \n",
              "3      স্বপ্ন বিদেশে গিয়েছিলেন ওমান প্রবাসী রানা আহমে...    Air  \n",
              "4      ইয়াঙ্গুন আন্তর্জাতিক বিমানবন্দরে  মে সন্ধ্যায় ...    Air  \n",
              "...                                                  ...    ...  \n",
              "11842  বান্দরবানের মার্মা বাজার ঘাটে সাঙ্গু নদে ডুবে ...  Water  \n",
              "11843  ফরিদপুরে পানিতে ডুবে মারা গেছেন গৃহবধূ ফরিদা ব...  Water  \n",
              "11844  ঢাকাবরিশালগামী লঞ্চ গতকাল শনিবার রাতে পা ফসকে ...  Water  \n",
              "11845  সেন্ট মার্টিন টেকনাফে ফেরার পথে পর্যটকবাহী জাহ...  Water  \n",
              "11846  সেন্ট মার্টিন টেকনাফে ফেরার পথে  নম্বর বইয়া এ...  Water  \n",
              "\n",
              "[11847 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cd342e6d-62d8-41e5-a5a0-67075462f7ce\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Description</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>পাখির আঘাতে ড্যাশ উড়োজাহাজের জরুরি অবতরণ</td>\n",
              "      <td>বিমান বাংলাদেশ এয়ারলাইনসের ড্যাশ মডেলের উড়োজাহ...</td>\n",
              "      <td>Air</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>চট্টগ্রামে বিমানে যান্ত্রিক ত্রুটি জরুরি অবতরণ</td>\n",
              "      <td>আকাশে ওড়ার  মিনিট চট্টগ্রাম শাহ আমানত আন্তর্জ...</td>\n",
              "      <td>Air</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>বিমানের ড্যাশ এইটের জরুরি অবতরণ</td>\n",
              "      <td>হজরত শাহজালাল আন্তর্জাতিক বিমানবন্দরে বিমান বা...</td>\n",
              "      <td>Air</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>উড়োজাহাজেই মৃত্যু ওমানপ্রবাসীর  ঘটেছি ফ্লাইটে</td>\n",
              "      <td>স্বপ্ন বিদেশে গিয়েছিলেন ওমান প্রবাসী রানা আহমে...</td>\n",
              "      <td>Air</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>অভ্র করবেন</td>\n",
              "      <td>ইয়াঙ্গুন আন্তর্জাতিক বিমানবন্দরে  মে সন্ধ্যায় ...</td>\n",
              "      <td>Air</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11842</th>\n",
              "      <td>11843</td>\n",
              "      <td>সাঙ্গু নদে ডুবে কলেজছাত্রের মৃত্যু</td>\n",
              "      <td>বান্দরবানের মার্মা বাজার ঘাটে সাঙ্গু নদে ডুবে ...</td>\n",
              "      <td>Water</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11843</th>\n",
              "      <td>11844</td>\n",
              "      <td>পানিতে ডুবে গৃহবধূর মৃত্যু</td>\n",
              "      <td>ফরিদপুরে পানিতে ডুবে মারা গেছেন গৃহবধূ ফরিদা ব...</td>\n",
              "      <td>Water</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11844</th>\n",
              "      <td>11845</td>\n",
              "      <td>উদ্ধার হননি লঞ্চ থে‌কে প‌ড়ে নারী</td>\n",
              "      <td>ঢাকাবরিশালগামী লঞ্চ গতকাল শনিবার রাতে পা ফসকে ...</td>\n",
              "      <td>Water</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11845</th>\n",
              "      <td>11846</td>\n",
              "      <td>উদ্ধার  জেলে খোঁজ মেলেনি নৌকার</td>\n",
              "      <td>সেন্ট মার্টিন টেকনাফে ফেরার পথে পর্যটকবাহী জাহ...</td>\n",
              "      <td>Water</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11846</th>\n",
              "      <td>11847</td>\n",
              "      <td>বঙ্গোপসাগরে জাহাজের ধাক্কায় ট্রলারডুবি পাঁচ ...</td>\n",
              "      <td>সেন্ট মার্টিন টেকনাফে ফেরার পথে  নম্বর বইয়া এ...</td>\n",
              "      <td>Water</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11847 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cd342e6d-62d8-41e5-a5a0-67075462f7ce')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cd342e6d-62d8-41e5-a5a0-67075462f7ce button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cd342e6d-62d8-41e5-a5a0-67075462f7ce');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc['cat_label'] = 0\n",
        "crime['cat_label'] = 1\n",
        "ent['cat_label'] = 3\n",
        "sports['cat_label'] = 4"
      ],
      "metadata": {
        "id": "6QTeyDgvXSXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "def label_encoder(df):\n",
        "\n",
        "    df[\"Label\"] = le.fit_transform(df[\"Label\"].astype(str))\n",
        "    label_map = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "    print(label_map)\n",
        "\n",
        "\n",
        "    df[\"Label\"] = le.fit_transform(df[\"Label\"].astype(str))\n",
        "    label_map = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "    print(label_map)\n",
        "\n",
        "label_encoder(acc)\n",
        "label_encoder(crime)\n",
        "label_encoder(ent)\n",
        "label_encoder(sports)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtX_xXx-dUIs",
        "outputId": "946e53ea-724f-48c6-b007-e00af788ddf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Air': 0, 'Blast': 1, 'Construction': 2, 'Electricity': 3, 'Fire': 4, 'Rail': 5, 'Road': 6, 'Water': 7, 'others': 8}\n",
            "{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8}\n",
            "{'CorruptionandFraud': 0, 'Drug': 1, 'Murder': 2, 'Others': 3, 'RapeandAbuse': 4, 'TheftRobberyAbduction': 5, 'Trafficking': 6}\n",
            "{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6}\n",
            "{'bollywood': 0, 'dhallywood': 1, 'hollywood': 2, 'music': 3, 'television': 4, 'tollywood': 5}\n",
            "{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5}\n",
            "{'athletics': 0, 'cricket': 1, 'football': 2, 'tennis': 3}\n",
            "{'0': 0, '1': 1, '2': 2, '3': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accY = acc[\"Label\"]\n",
        "accX = acc['Description']\n",
        "\n",
        "crimeY = crime[\"Label\"]\n",
        "crimeX = crime[\"Description\"]\n",
        "\n",
        "entY = ent[\"Label\"]\n",
        "entX = ent[\"Description\"]\n",
        "\n",
        "sportsY = sports[\"Label\"]\n",
        "sportsX = sports[\"Description\"]"
      ],
      "metadata": {
        "id": "5lj7WJkTjK06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accX_train, accX_test, accy_train, accy_test = train_test_split(accX, accY, test_size = 0.1, random_state = 5 , stratify=accY)\n",
        "crimeX_train, crimeX_test, crimey_train, crimey_test = train_test_split(crimeX, crimeY, test_size = 0.1, random_state = 4,  stratify=crimeY)\n",
        "entX_train, entX_test, enty_train, enty_test = train_test_split(entX, entY, test_size = 0.1, random_state = 5, stratify=entY)\n",
        "sportsX_train, sportsX_test, sportsy_train, sportsy_test = train_test_split(sportsX, sportsY, test_size = 0.1, random_state = 5, stratify=sportsY)"
      ],
      "metadata": {
        "id": "ERcy6Ak6jVVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords =list(open('/content/drive/My Drive/Data/Stopwords.txt', mode='r', encoding='UTF-8'))\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    removed = []\n",
        "    stop_words = stopwords\n",
        "    tokens = word_tokenize(text)\n",
        "    for i in range(len(tokens)):\n",
        "        if tokens[i] not in stop_words:\n",
        "            removed.append(tokens[i])\n",
        "    return \" \".join(removed)\n",
        "\n",
        "acc['Description'] = acc['Description'].apply(lambda x: remove_stopwords(str(x)))\n",
        "acc['Title'] = acc['Title'].apply(lambda x: remove_stopwords(str(x)))\n",
        "acc['New_length'] = acc['Description'].apply(lambda x: len(str(x)))\n",
        "acc = acc.astype({'New_length': np.int})\n",
        "\n",
        "crime['Description'] = crime['Description'].apply(lambda x: remove_stopwords(str(x)))\n",
        "crime['Title'] = crime['Title'].apply(lambda x: remove_stopwords(str(x)))\n",
        "crime['New_length'] = crime['Description'].apply(lambda x: len(str(x)))\n",
        "crime = crime.astype({'New_length': np.int})\n",
        "\n",
        "ent['Description'] = ent['Description'].apply(lambda x: remove_stopwords(str(x)))\n",
        "ent['Title'] = ent['Title'].apply(lambda x: remove_stopwords(str(x)))\n",
        "ent['New_length'] = ent['Description'].apply(lambda x: len(str(x)))\n",
        "ent = ent.astype({'New_length': np.int})\n",
        "\n",
        "sports['Description'] = sports['Description'].apply(lambda x: remove_stopwords(str(x)))\n",
        "sports['Title'] = sports['Title'].apply(lambda x: remove_stopwords(str(x)))\n",
        "sports['New_length'] = sports['Description'].apply(lambda x: len(str(x)))\n",
        "sports = sports.astype({'New_length': np.int})"
      ],
      "metadata": {
        "id": "5JtKXmCajgVy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}